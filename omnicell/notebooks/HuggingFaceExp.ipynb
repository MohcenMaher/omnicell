{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "from omnicell.data.loader import DataLoader, DatasetDetails\n",
    "import torch \n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import transformers\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache location: /om/user/opitcho/.cache/huggingface/hub\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import  TRANSFORMERS_CACHE\n",
    "print(f\"Cache location: {TRANSFORMERS_CACHE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model found in cache: True\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import try_to_load_from_cache\n",
    "model_name = \"chaoyi-wu/PMC_LLAMA_7B\"\n",
    "# Try to find a model file\n",
    "cache_result = try_to_load_from_cache(model_name, filename=\"pytorch_model.bin\")\n",
    "print(f\"Model found in cache: {cache_result is not None}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache size: 2.08 GB\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "cache_path = Path(TRANSFORMERS_CACHE)\n",
    "cache_size = sum(f.stat().st_size for f in cache_path.glob('**/*') if f.is_file())\n",
    "print(f\"Cache size: {cache_size / (1024*1024*1024):.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache location: /om/user/opitcho/.cache/huggingface/hub\n",
      "Successfully wrote to cache directory!\n",
      "Successfully removed test file\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import TRANSFORMERS_CACHE\n",
    "import tempfile\n",
    "\n",
    "# Print cache location first\n",
    "print(f\"Cache location: {TRANSFORMERS_CACHE}\")\n",
    "\n",
    "# Try to create a test file in the cache directory\n",
    "test_file_path = os.path.join(TRANSFORMERS_CACHE, \"test_write.txt\")\n",
    "try:\n",
    "    with open(test_file_path, \"w\") as f:\n",
    "        f.write(\"Test write permission\")\n",
    "    print(\"Successfully wrote to cache directory!\")\n",
    "    \n",
    "    # Clean up the test file\n",
    "    os.remove(test_file_path)\n",
    "    print(\"Successfully removed test file\")\n",
    "except PermissionError:\n",
    "    print(\"ERROR: No write permission to cache directory\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "121d0d15f554441682d2c51e913f6d74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Clear the cache first\n",
    "transformers.utils.move_cache()\n",
    "transformers.utils.logging.set_verbosity_debug()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4300d175c4146b08435edff257e5b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/141 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "059aaa9485a545adb05c1c75bce5dcf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3e001e7230b46f28d43ef92bd1c6641",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f568814123e743b386c28ee7d6cc5fd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/141 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.model from cache at /om/user/opitcho/.cache/huggingface/hub/models--chaoyi-wu--PMC_LLAMA_7B/snapshots/6caf5c19bdcd157f9d9a7d374be66d7b61d75351/tokenizer.model\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /om/user/opitcho/.cache/huggingface/hub/models--chaoyi-wu--PMC_LLAMA_7B/snapshots/6caf5c19bdcd157f9d9a7d374be66d7b61d75351/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /om/user/opitcho/.cache/huggingface/hub/models--chaoyi-wu--PMC_LLAMA_7B/snapshots/6caf5c19bdcd157f9d9a7d374be66d7b61d75351/tokenizer_config.json\n",
      "loading file tokenizer.json from cache at None\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d272cc9c9764f759c1d81de40848305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/552 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "286739d0d07a426daad4fc1c57829280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/552 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /om/user/opitcho/.cache/huggingface/hub/models--chaoyi-wu--PMC_LLAMA_7B/snapshots/6caf5c19bdcd157f9d9a7d374be66d7b61d75351/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"./LLAMA_Model/llama-7b-hf\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"max_sequence_length\": 2048,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": -1,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.45.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e194060d00d842a5b58158229ccb8ccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file pytorch_model.bin from cache at /om/user/opitcho/.cache/huggingface/hub/models--chaoyi-wu--PMC_LLAMA_7B/snapshots/6caf5c19bdcd157f9d9a7d374be66d7b61d75351/pytorch_model.bin.index.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6a3c7ccf07e4646a03fb1c75b9f50b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "836e3c2faba047ffad70593c1c9802dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00003.bin:   0%|          | 0.00/9.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42576b60cc3d4d67b788ccc08bc99783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00003.bin:   0%|          | 0.00/9.89G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3fb0407989a46e9a6d3bdb8966ff0a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00003-of-00003.bin:   0%|          | 0.00/7.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/orcd/archive/abugoot/001/Projects/opitcho/miniforge3/envs/huggingface/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:579: UserWarning: `pad_token_id` should be positive but got -1. This will cause errors when batch generating, if there is padding. Please set `pad_token_id` explicitly as `model.generation_config.pad_token_id=PAD_TOKEN_ID` to avoid errors in generation\n",
      "  warnings.warn(\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": -1\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e20f7c64989447baaec20cf1d1a62ef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "All the weights of LlamaForCausalLM were initialized from the model checkpoint at chaoyi-wu/PMC_LLAMA_7B.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bc86cf34e064140be6e51489164b9ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file generation_config.json from cache at /om/user/opitcho/.cache/huggingface/hub/models--chaoyi-wu--PMC_LLAMA_7B/snapshots/6caf5c19bdcd157f9d9a7d374be66d7b61d75351/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.LlamaTokenizer.from_pretrained(\n",
    "    'chaoyi-wu/PMC_LLAMA_7B',\n",
    "    local_files_only=False,\n",
    "    force_download=True\n",
    ")\n",
    "\n",
    "model = transformers.LlamaForCausalLM.from_pretrained(\n",
    "    'chaoyi-wu/PMC_LLAMA_7B',\n",
    "    local_files_only=False,\n",
    "    force_download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'logits', 'past_key_values', 'hidden_states', 'attentions'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [\"A third veryvery very very very very very very very long sentence\"]\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "# Get the embeddings\n",
    "outputs = model(**inputs)\n",
    "\n",
    "outputs.__dict__.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32000])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The pooled output represents the entire sequence\n",
    "outputs.logits\n",
    "\n",
    "#Squeeze the output to remove the batch dimension\n",
    "outputs.logits.squeeze().sum(axis = 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 10762, 29940, 1718, 29896], 'attention_mask': [1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"IFNAR1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
