{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import scvi\n",
    "import torch\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import os\n",
    "\n",
    "import scanpy as sc\n",
    "\n",
    "from torchcfm.conditional_flow_matching import *\n",
    "import scanpy as sc\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load some data\n",
    "adata = sc.read_h5ad('/orcd/archive/abugoot/001/Projects/dlesman/datasets/kaggle_HVG.h5ad')\n",
    "\n",
    "# here we set up the train/eval and control/pert sets\n",
    "# set the idx of the controls\n",
    "control_idx = adata.obs['sm_name'] == 'Dimethyl Sulfoxide'\n",
    "# set the idx of the perts (currently just \"all not control\")\n",
    "pert_idx = adata.obs['sm_name'] != 'Dimethyl Sulfoxide'\n",
    "# set the hold out cell-type/pert\n",
    "eval_cell_idx = adata.obs.cell_type == 'B cells'\n",
    "eval_pert_idx = adata.obs['sm_name'] == 'Belinostat'\n",
    "eval_idx = eval_cell_idx & eval_pert_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we set up our embeddings for cfm\n",
    "# this is just so everything lives in obsm for the for loop below\n",
    "adata.obsm[\"X\"] = adata.X\n",
    "\n",
    "# this is an example of how we can embed something using just the train idxs\n",
    "# and then run fm on that embedding\n",
    "# embedder = PCA(n_components=30).fit(adata.X[(control_idx | pert_idx) & ~eval_idx])\n",
    "# adata.obsm[\"X_pca\"] = embedder.transform(adata.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we set up the perts\n",
    "import pandas as pd\n",
    "perts = pd.get_dummies(adata.obs['sm_name']).values.astype(float)\n",
    "pert_ids = perts.argmax(axis=1)\n",
    "# this is the \"identity featurization\"; we can swap this matrix for\n",
    "# any latent representation of perturbations we want but this is \n",
    "# a non-parametric featurization for right now\n",
    "pert_mat = np.eye(pert_ids.max() + 1).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_types = pd.get_dummies(adata.obs.cell_type).values.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = adata.obsm['X']\n",
    "\n",
    "# set train and eval split\n",
    "control_train = X[control_idx & ~eval_idx]\n",
    "pert_train = X[pert_idx & ~eval_idx]\n",
    "pert_ids_train =  pert_ids[pert_idx & ~eval_idx]\n",
    "control_cell_types = cell_types[control_idx & ~eval_idx]\n",
    "pert_cell_types = cell_types[pert_idx & ~eval_idx]\n",
    "\n",
    "control_eval = X[control_idx & eval_cell_idx]\n",
    "pert_eval = X[eval_idx]\n",
    "pert_ids_eval = pert_ids[eval_idx]\n",
    "\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/orcd/archive/abugoot/001/Projects/njwfish/dsbm/DSBM_Gaussian.py:656: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"gaussian.yaml\")\n"
     ]
    }
   ],
   "source": [
    "from DSBM_Gaussian import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "cfg = Namespace(net_name=\"mlp_small\", num_steps=20, sigma=1, inner_iters=10000, outer_iters=40,\n",
    "               activation_fn=torch.nn.SiLU, model_name=\"dsb\", first_coupling=\"ref\",  fb_sequence=['b', 'f'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = torch.tensor(control_train)\n",
    "x1 = torch.tensor(pert_train)\n",
    "x1 = x0[:x1.shape[0]]\n",
    "dim = x0.shape[1]\n",
    "x_pairs = torch.stack([x0, x1], dim=1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: <530768>\n",
      "Iteration 1/40 b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [25:07<00:00,  6.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2/40 f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [50:35<00:00,  3.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3/40 b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [45:08<00:00,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4/40 f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [41:11<00:00,  4.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5/40 b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [39:53<00:00,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6/40 f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████| 10000/10000 [38:55<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7/40 b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████| 10000/10000 [38:54<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8/40 f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████| 10000/10000 [38:57<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9/40 b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████| 10000/10000 [39:06<00:00,  4.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10/40 f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████| 10000/10000 [39:17<00:00,  4.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11/40 b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████| 10000/10000 [36:38<00:00,  4.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12/40 f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|███▍                      | 1331/10000 [04:08<26:55,  5.37it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Loss is nan",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 55\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     prev_model \u001b[38;5;241m=\u001b[39m model_list[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 55\u001b[0m model, loss_curve \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_pairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minner_iters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprev_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_it\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_it\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m model_list\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfb\u001b[39m\u001b[38;5;124m'\u001b[39m: fb, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: copy\u001b[38;5;241m.\u001b[39mdeepcopy(model)\u001b[38;5;241m.\u001b[39meval()})\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m: \u001b[38;5;66;03m# hasattr(model, \"sample_sde\"):\u001b[39;00m\n",
      "File \u001b[0;32m/orcd/archive/abugoot/001/Projects/njwfish/dsbm/DSBM_Gaussian.py:176\u001b[0m, in \u001b[0;36mtrain_dsb_ipf\u001b[0;34m(dsb_ipf, x_pairs, batch_size, inner_iters, fb, first_it, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misnan(loss)\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m--> 176\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss is nan\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    177\u001b[0m   \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    179\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[0;31mValueError\u001b[0m: Loss is nan"
     ]
    }
   ],
   "source": [
    "net_split = cfg.net_name.split(\"_\")\n",
    "if net_split[0] == \"mlp\":\n",
    "    if net_split[1] == \"small\":\n",
    "        net_fn = partial(ScoreNetwork, input_dim=dim+1, layer_widths=[128, 128, dim], activation_fn=cfg.activation_fn())    # hydra.utils.get_method(cfg.activation_fn))    # \n",
    "    else:\n",
    "        net_fn = partial(ScoreNetwork, input_dim=dim+1, layer_widths=[256, 256, dim], activation_fn=cfg.activation_fn())    # hydra.utils.get_method(cfg.activation_fn))    # \n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "num_steps = cfg.num_steps\n",
    "sigma = cfg.sigma\n",
    "inner_iters = cfg.inner_iters\n",
    "outer_iters = cfg.outer_iters\n",
    "\n",
    "if cfg.model_name == \"dsb\":\n",
    "    model = DSB(net_fwd=net_fn().to(device), \n",
    "                            net_bwd=net_fn().to(device), \n",
    "                            num_steps=num_steps, sig=sigma)\n",
    "    train_fn = train_dsb_ipf\n",
    "    print(f\"Number of parameters: <{sum(p.numel() for p in model.net_fwd.parameters() if p.requires_grad)}>\")\n",
    "elif cfg.model_name == \"dsbm\":\n",
    "    model = DSBM(net_fwd=net_fn().to(device), \n",
    "                                net_bwd=net_fn().to(device), \n",
    "                                num_steps=num_steps, sig=sigma, first_coupling=cfg.first_coupling)\n",
    "    train_fn = train_dsbm\n",
    "    print(f\"Number of parameters: <{sum(p.numel() for p in model.net_fwd.parameters() if p.requires_grad)}>\")\n",
    "elif cfg.model_name == \"sbcfm\":\n",
    "    model = SBCFM(net=net_fn().to(device), \n",
    "                                num_steps=num_steps, sig=sigma)\n",
    "    train_fn = train_flow_model\n",
    "    print(f\"Number of parameters: <{sum(p.numel() for p in model.net.parameters() if p.requires_grad)}>\")\n",
    "elif cfg.model_name == \"rectifiedflow\":\n",
    "    model = RectifiedFlow(net=net_fn().to(device), \n",
    "                                                num_steps=num_steps, sig=None)\n",
    "    train_fn = train_flow_model\n",
    "    print(f\"Number of parameters: <{sum(p.numel() for p in model.net.parameters() if p.requires_grad)}>\")\n",
    "else:\n",
    "    raise ValueError(\"Wrong model_name!\")\n",
    "\n",
    "\n",
    "# Training loop\n",
    "# first_it = True\n",
    "model_list = []\n",
    "it = 1\n",
    "\n",
    "# assert outer_iters % len(cfg.fb_sequence) == 0\n",
    "while it <= outer_iters:\n",
    "    for fb in cfg.fb_sequence:\n",
    "        print(f\"Iteration {it}/{outer_iters} {fb}\")\n",
    "        first_it = (it == 1)\n",
    "        if first_it:\n",
    "            prev_model = None\n",
    "        else:\n",
    "            prev_model = model_list[-1][\"model\"].eval()\n",
    "        model, loss_curve = train_fn(model, x_pairs, batch_size, inner_iters, prev_model=prev_model, fb=fb, first_it=first_it)\n",
    "        model_list.append({'fb': fb, 'model': copy.deepcopy(model).eval()})\n",
    "    \n",
    "        if False: # hasattr(model, \"sample_sde\"):\n",
    "            draw_plot(partial(model.sample_sde, zstart=x_test_dict[fb], fb=fb, first_it=first_it), z0=x_test_dict['f'], z1=x_test_dict['b'])\n",
    "            plt.savefig(f\"{it}-{fb}.png\")\n",
    "            plt.close()\n",
    "\n",
    "            # Evaluation\n",
    "            optimal_result_dict = {'mean': -a, 'var': 1, 'cov': (np.sqrt(5) - 1) / 2}\n",
    "            result_list = {k: [] for k in optimal_result_dict.keys()}\n",
    "            for i in range(it):\n",
    "                traj = model_list[i]['model'].sample_sde(zstart=x1_test, fb='b')\n",
    "                result_list['mean'].append(traj[-1].mean(0).mean(0).item())\n",
    "                result_list['var'].append(traj[-1].var(0).mean(0).item())\n",
    "                result_list['cov'].append(torch.cov(torch.cat([traj[0], traj[-1]], dim=1).T)[dim:, :dim].diag().mean(0).item())\n",
    "            for i, k in enumerate(result_list.keys()):\n",
    "                plt.plot(result_list[k], label=f\"{cfg.model_name}-{cfg.net_name}\")\n",
    "                plt.plot(np.arange(outer_iters), optimal_result_dict[k] * np.ones(outer_iters), label=\"optimal\", linestyle=\"--\")\n",
    "                plt.title(k.capitalize())\n",
    "                if i == 0:\n",
    "                    plt.legend()\n",
    "                plt.savefig(f\"convergence_{k}.png\")\n",
    "                plt.close()\n",
    "            \n",
    "            result_list_100 = {k: [] for k in optimal_result_dict.keys()}\n",
    "            for i in range(it):\n",
    "                traj_100 = model_list[i]['model'].sample_sde(zstart=x1_test, fb='b', N=100)\n",
    "                result_list_100['mean'].append(traj_100[-1].mean(0).mean(0).item())\n",
    "                result_list_100['var'].append(traj_100[-1].var(0).mean(0).item())\n",
    "                result_list_100['cov'].append(torch.cov(torch.cat([traj_100[0], traj_100[-1]], dim=1).T)[dim:, :dim].diag().mean(0).item())\n",
    "        \n",
    "        if False: # hasattr(model, \"sample_ode\"):\n",
    "            draw_plot(partial(model.sample_ode, zstart=x_test_dict[fb], fb=fb, first_it=first_it), z0=x_test_dict['f'], z1=x_test_dict['b'])\n",
    "            plt.savefig(f\"{it}-{fb}-ode.png\")\n",
    "            plt.close()\n",
    "\n",
    "            # Evaluation\n",
    "            optimal_result_dict_ode = {'mean': -a, 'var': 1}\n",
    "            result_list_ode = {k: [] for k in optimal_result_dict_ode.keys()}\n",
    "            for i in range(it):\n",
    "                traj_ode = model_list[i]['model'].sample_ode(zstart=x1_test, fb='b')\n",
    "                result_list_ode['mean'].append(traj_ode[-1].mean(0).mean(0).item())\n",
    "                result_list_ode['var'].append(traj_ode[-1].var(0).mean(0).item())\n",
    "            for i, k in enumerate(result_list_ode.keys()):\n",
    "                plt.plot(result_list_ode[k], label=f\"{cfg.model_name}-{cfg.net_name}-ode\")\n",
    "                plt.plot(np.arange(outer_iters), optimal_result_dict_ode[k] * np.ones(outer_iters), label=\"optimal\", linestyle=\"--\")\n",
    "                plt.title(k.capitalize())\n",
    "                if i == 0:\n",
    "                    plt.legend()\n",
    "                plt.savefig(f\"convergence_{k}-ode.png\")\n",
    "                plt.close()\n",
    "            \n",
    "            result_list_ode_100 = {k: [] for k in optimal_result_dict_ode.keys()}\n",
    "            for i in range(it):\n",
    "                traj_ode_100 = model_list[i]['model'].sample_ode(zstart=x1_test, fb='b', N=100)\n",
    "                result_list_ode_100['mean'].append(traj_ode_100[-1].mean(0).mean(0).item())\n",
    "                result_list_ode_100['var'].append(traj_ode_100[-1].var(0).mean(0).item())\n",
    "\n",
    "        # first_it = False\n",
    "        it += 1\n",
    "\n",
    "        if it > outer_iters:\n",
    "            break\n",
    "\n",
    "torch.save([{'fb': m['fb'], 'model': m['model'].state_dict()} for m in model_list], \"model_list.pt\")\n",
    "\n",
    "if hasattr(model, \"sample_sde\"):\n",
    "    df_result = pd.DataFrame(result_list)\n",
    "    df_result_100 = pd.DataFrame(result_list_100)\n",
    "    df_result.to_csv('df_result.csv')\n",
    "    df_result.to_pickle('df_result.pkl')\n",
    "    df_result_100.to_csv('df_result_100.csv')\n",
    "    df_result_100.to_pickle('df_result_100.pkl')\n",
    "\n",
    "    # Trajectory\n",
    "    np.save(\"traj.npy\", torch.stack(traj, dim=1).detach().cpu().numpy())\n",
    "    np.save(\"traj_100.npy\", torch.stack(traj_100, dim=1).detach().cpu().numpy())\n",
    "\n",
    "if hasattr(model, \"sample_ode\"):\n",
    "    df_result_ode = pd.DataFrame(result_list_ode)\n",
    "    df_result_ode_100 = pd.DataFrame(result_list_ode_100)\n",
    "    df_result_ode.to_csv('df_result_ode.csv')\n",
    "    df_result_ode.to_pickle('df_result_ode.pkl')\n",
    "    df_result_ode_100.to_csv('df_result_ode_100.csv')\n",
    "    df_result_ode_100.to_pickle('df_result_ode_100.pkl')\n",
    "\n",
    "    # Trajectory\n",
    "    np.save(\"traj_ode.npy\", torch.stack(traj_ode, dim=1).detach().cpu().numpy())\n",
    "    np.save(\"traj_ode_100.npy\", torch.stack(traj_ode_100, dim=1).detach().cpu().numpy())\n",
    "\n",
    "return {}, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsbm",
   "language": "python",
   "name": "dsbm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
