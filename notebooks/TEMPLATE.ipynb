{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-16 13:35:18,578 - INFO - Loading training data at path: /orcd/data/omarabu/001/Omnicell_datasets/repogle_k562_essential_raw/K562_essential_raw_singlecell_01.h5ad\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-16 13:35:23,844 - INFO - Loaded unpreprocessed data, # of data points: 310385, # of genes: 8563.\n",
      "2025-01-16 13:35:23,845 - INFO - Preprocessing training data\n",
      "/orcd/data/omarabu/001/njwfish/omnicell/notebooks/../omnicell/data/loader.py:119: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  embedding = torch.load(f\"{dataset_details.folder_path}/{self.gene_embedding_name}.pt\")\n",
      "2025-01-16 13:35:23,865 - INFO - Removing observations with perturbations not in the dataset as a column\n",
      "/orcd/data/omarabu/001/njwfish/omnicell/notebooks/../omnicell/data/loader.py:151: ImplicitModificationWarning: Setting element `.obsm['embedding']` of view, initializing view as actual.\n",
      "  adata.obsm[\"embedding\"] = adata.X.toarray().astype('float32')\n",
      "2025-01-16 13:36:16,763 - INFO - Doing OOD split\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded:\n",
      "- Number of cells: 279630\n",
      "- Input dimension: 8563\n",
      "- Number of perturbations: 1850\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import yaml\n",
    "import torch\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the path to the directory containing the omnicell package\n",
    "# Assuming the omnicell package is in the parent directory of your notebook\n",
    "sys.path.append('..')  # Adjust this path as needed\n",
    "\n",
    "import yaml\n",
    "import torch\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from omnicell.config.config import Config\n",
    "from omnicell.data.loader import DataLoader\n",
    "from omnicell.constants import PERT_KEY, GENE_EMBEDDING_KEY, CONTROL_PERT\n",
    "# from omnicell.models.model_factory import get_model\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configure paths\n",
    "MODEL_CONFIG = \"/orcd/data/omarabu/001/njwfish/omnicell/configs/models/sclambda_large_no_clip.yaml\"\n",
    "ETL_CONFIG = \"/orcd/data/omarabu/001/njwfish/omnicell/configs/ETL/no_preprocessing.yaml\"  # Change this to your desired ETL config\n",
    "SPLIT_CONFIG = \"/orcd/data/omarabu/001/njwfish/omnicell/configs/repogle_k562_essential_raw/random_splits/rs_accP_k562_ood_ss:ns_20_2_most_pert_0.1/split_0/split_config.yaml\"\n",
    "EVAL_CONFIG = None  # Set this if you want to run evaluations\n",
    "\n",
    "# Load configuration\n",
    "config = Config.from_yamls(MODEL_CONFIG, ETL_CONFIG, SPLIT_CONFIG, EVAL_CONFIG)\n",
    "config.etl_config['gene_embedding'] = 'bioBERT'\n",
    "config.etl_config['drop_unmatched_perts'] = True\n",
    "# Set up device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize data loader and load training data\n",
    "loader = DataLoader(config)\n",
    "adata, pert_rep_map = loader.get_training_data()\n",
    "\n",
    "# Get dimensions and perturbation IDs\n",
    "input_dim = adata.obsm['embedding'].shape[1]\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "pert_ids = adata.obs[PERT_KEY].unique()\n",
    "gene_emb_dim = adata.varm[GENE_EMBEDDING_KEY].shape[1] if GENE_EMBEDDING_KEY in adata.varm else None\n",
    "\n",
    "print(f\"Data loaded:\")\n",
    "print(f\"- Number of cells: {adata.shape[0]}\")\n",
    "print(f\"- Input dimension: {input_dim}\")\n",
    "print(f\"- Number of perturbations: {len(pert_ids)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omnicell.constants import PERT_KEY, GENE_EMBEDDING_KEY, CONTROL_PERT, CELL_KEY\n",
    "\n",
    "import scanpy as sc\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "# After the existing imports, add:\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "def expected_distribute_shift(ctrl_cells, shift_pred):\n",
    "    cell_fractions = ctrl_cells.sum(axis=1) / ctrl_cells.sum()\n",
    "    Z = ctrl_cells + shift_pred[None, :] * cell_fractions[:, None] * ctrl_cells.shape[0]\n",
    "    return Z\n",
    "\n",
    "def distribute_shift(ctrl_cells, mean_shift):\n",
    "    \"\"\"\n",
    "    Distribute the global per-gene difference (sum_diff[g]) across cells in proportion\n",
    "    to the cell's existing counts for that gene. \n",
    "    \"\"\" \n",
    "    ctrl_cells = ctrl_cells.copy()\n",
    "    sum_shift = (mean_shift * ctrl_cells.shape[0]).astype(int)\n",
    "\n",
    "    n_cells, n_genes = ctrl_cells.shape\n",
    "\n",
    "    #For each gene, distribute sum_diff[g] using a single multinomial draw\n",
    "    for g in range(n_genes):\n",
    "        diff = int(sum_shift[g])\n",
    "        if diff == 0:\n",
    "            continue  \n",
    "\n",
    "        # Current counts for this gene across cells\n",
    "        gene_counts = ctrl_cells[:, g]\n",
    "\n",
    "        current_total = gene_counts.sum().astype(np.float64)\n",
    "        \n",
    "\n",
    "        # Probabilities ~ gene_counts / current_total\n",
    "        p = gene_counts / current_total\n",
    "\n",
    "\n",
    "        if diff > 0:\n",
    "            # We want to add `diff` counts\n",
    "            draws = np.random.multinomial(diff, p)  # shape: (n_cells,)\n",
    "            \n",
    "            ctrl_cells[:, g] = gene_counts + draws\n",
    "        else:\n",
    "            if current_total <= 0:\n",
    "                continue\n",
    "\n",
    "            # We want to remove `abs(diff)` counts\n",
    "            amt_to_remove = abs(diff)\n",
    "\n",
    "            to_remove = min(amt_to_remove, current_total)\n",
    "            draws = np.random.multinomial(to_remove, p)\n",
    "            # Subtract, then clamp\n",
    "            updated = gene_counts - draws\n",
    "            updated[updated < 0] = 0\n",
    "            ctrl_cells[:, g] = updated\n",
    "\n",
    "    return ctrl_cells\n",
    "\n",
    "def fit_supervised_model(X, Y, model_type='linear', **kwargs):\n",
    "    \"\"\"\n",
    "    Fit a supervised model based on the specified model type.\n",
    "    \n",
    "    Args:\n",
    "        X: Input features (gene embeddings)\n",
    "        Y: Target values (perturbation effects)\n",
    "        model_type: Type of model to fit ('linear', 'ridge', 'lasso', 'elastic_net', 'rf', 'svr')\n",
    "        **kwargs: Additional arguments to pass to the model constructor\n",
    "    \n",
    "    Returns:\n",
    "        fitted model, training MSE, R2 score\n",
    "    \"\"\"\n",
    "    models = {\n",
    "        'linear': LinearRegression,\n",
    "        'ridge': Ridge,\n",
    "        'lasso': Lasso,\n",
    "        'elastic_net': ElasticNet,\n",
    "        'rf': RandomForestRegressor,\n",
    "        'svr': SVR\n",
    "    }\n",
    "    \n",
    "    if model_type not in models:\n",
    "        raise ValueError(f\"Model type {model_type} not supported. Choose from {list(models.keys())}\")\n",
    "    \n",
    "    model = models[model_type](**kwargs)\n",
    "    model.fit(X, Y)\n",
    "    \n",
    "    # Make predictions and calculate metrics\n",
    "    Y_pred = model.predict(X)\n",
    "    mse = mean_squared_error(Y, Y_pred)\n",
    "    r2 = r2_score(Y, Y_pred)\n",
    "    \n",
    "    return model, mse, r2\n",
    "\n",
    "def compute_cell_type_means(adata, cell_type):\n",
    "    \"\"\"Compute perturbation effect embeddings for a specific cell type\"\"\"\n",
    "    # Filter data for this cell type\n",
    "    cell_type_data = adata[adata.obs[CELL_KEY] == cell_type]\n",
    "    \n",
    "    # Compute control mean for this cell type\n",
    "    ctrl_mean = np.mean(\n",
    "        cell_type_data[cell_type_data.obs[PERT_KEY] == CONTROL_PERT].X, axis=0\n",
    "    )\n",
    "    \n",
    "    # Convert to dense array if sparse\n",
    "    X = cell_type_data.X.toarray() if scipy.sparse.issparse(cell_type_data.X) else cell_type_data.X\n",
    "    \n",
    "    # Create dataframe\n",
    "    df = pd.DataFrame(X, index=cell_type_data.obs.index)\n",
    "    df['perturbation'] = cell_type_data.obs[PERT_KEY].values\n",
    "    \n",
    "    # Compute means per perturbation\n",
    "    pert_means = df.groupby('perturbation').mean()\n",
    "    \n",
    "    # Compute deltas from control\n",
    "    pert_deltas = pd.DataFrame(pert_means.values - ctrl_mean, index=pert_means.index)\n",
    "    pert_deltas_dict = {\n",
    "        pert: np.array(means) \n",
    "        for pert, means in pert_deltas.iterrows() \n",
    "        if pert != CONTROL_PERT\n",
    "    }\n",
    "    \n",
    "    return ctrl_mean, pert_deltas_dict\n",
    "\n",
    "class MeanPredictor():\n",
    "\n",
    "    def __init__(self, adata: sc.AnnData, model_config: dict):\n",
    "        self.model = None\n",
    "        self.model_type = model_config['model_type']\n",
    "        self.pca_gene_embeddings = model_config['pca_gene_embeddings']\n",
    "        self.pca_gene_embeddings_components = model_config['pca_gene_embeddings_components']\n",
    "        self.total_adata = adata\n",
    "        self.gene_emb = None\n",
    "\n",
    "    def train(self, adata: sc.AnnData):\n",
    "        if self.pca_gene_embeddings:\n",
    "            pca = PCA(n_components=self.pca_gene_embeddings_components)\n",
    "            gene_emb_temp = pca.fit_transform(adata.varm[GENE_EMBEDDING_KEY])\n",
    "        else:\n",
    "            gene_emb_temp = adata.varm[GENE_EMBEDDING_KEY]\n",
    "\n",
    "        gene_emb = {}\n",
    "        for i, g in enumerate(adata.var_names):\n",
    "            gene_emb[g] = gene_emb_temp[i]\n",
    "        \n",
    "        self.gene_emb = gene_emb\n",
    "        # Get unique cell types\n",
    "        cell_types = adata.obs[CELL_KEY].unique()\n",
    "\n",
    "        # Compute embeddings for each cell type\n",
    "        Xs = []\n",
    "        Ys = []\n",
    "        for cell_type in cell_types:\n",
    "            ctrl_mean, pert_deltas_dict = compute_cell_type_means(adata, cell_type)\n",
    "            \n",
    "            # Get perturbation IDs for this cell type\n",
    "            idxs = pert_deltas_dict.keys()\n",
    "            \n",
    "            # Create feature matrix X and target matrix Y\n",
    "            Y = np.array([pert_deltas_dict[pert] for pert in idxs])\n",
    "            X = np.array([gene_emb[g] for g in idxs])\n",
    "\n",
    "            # Store the embeddings\n",
    "            Xs.append(X)\n",
    "            Ys.append(Y)\n",
    "\n",
    "        # Now you can train a model for each cell type\n",
    "        X = np.concatenate(Xs)\n",
    "        Y = np.concatenate(Ys)\n",
    "        self.model, mse, r2 = fit_supervised_model(X, Y, model_type=self.model_type)\n",
    "        \n",
    "    def make_predict(self, adata: sc.AnnData, pert_id: str, cell_type: str) -> np.ndarray:\n",
    "        ctrl_cells = adata[(adata.obs[PERT_KEY] == CONTROL_PERT) & (adata.obs[CELL_KEY] == cell_type)].X.toarray()\n",
    "        X_new = np.array(self.gene_emb[pert_id].reshape(1, -1))\n",
    "        shift_pred = np.array(self.model.predict(X_new)).flatten()\n",
    "        return distribute_shift(ctrl_cells, shift_pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64908/781781914.py:121: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  pert_means = df.groupby('perturbation').mean()\n"
     ]
    }
   ],
   "source": [
    "model_config = {\n",
    "    'model_type': 'linear',\n",
    "    'pca_gene_embeddings': True,\n",
    "    'pca_gene_embeddings_components': 10\n",
    "}\n",
    "\n",
    "model = MeanPredictor(adata, model_config)\n",
    "model.train(adata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift_pred = pert_deltas_dict[pert_id]\n",
    "ctrl_cells = adata[(adata.obs[PERT_KEY] == CONTROL_PERT) & (adata.obs[CELL_KEY] == cell_type)].X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = model.make_predict(adata, 'SLC39A9', 'k562')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[18.943586,  0.      ,  0.      , ..., 30.510262,  0.      ,\n",
       "          0.      ],\n",
       "        [21.323915,  0.      ,  0.      , ..., 34.868988,  0.      ,\n",
       "          0.      ],\n",
       "        [18.176373,  0.      ,  0.      , ..., 29.274597,  0.      ,\n",
       "          0.      ],\n",
       "        ...,\n",
       "        [21.275522,  0.      ,  0.      , ..., 34.26604 ,  0.      ,\n",
       "          0.      ],\n",
       "        [21.526276,  0.      ,  0.      , ..., 35.150177,  0.      ,\n",
       "          0.      ],\n",
       "        [21.763897,  0.      ,  0.      , ..., 35.46508 ,  0.      ,\n",
       "          0.      ]], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pert\n",
       "ctrl       10691\n",
       "RPL3        1996\n",
       "NCBP2        992\n",
       "KIF11        974\n",
       "SLC39A9      752\n",
       "           ...  \n",
       "NUP155         7\n",
       "POLR3A         6\n",
       "SEC62          5\n",
       "RBM22          5\n",
       "POT1           5\n",
       "Name: count, Length: 1850, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.obs[PERT_KEY].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00410382, -0.01693404, -0.02247995, ...,  0.02091249,\n",
       "         0.02567016,  0.0138661 ],\n",
       "       [ 0.01048473,  0.02137071, -0.0019055 , ..., -0.01459742,\n",
       "        -0.02456123,  0.0438426 ],\n",
       "       [ 0.05892141, -0.03006956, -0.29437995, ...,  0.03454808,\n",
       "        -0.01858041, -0.04018317],\n",
       "       ...,\n",
       "       [ 0.05377207, -0.0211549 ,  0.00326538, ...,  0.01787109,\n",
       "        -0.01472393,  0.00791901],\n",
       "       [-0.01146409, -0.01612045, -0.00243032, ...,  0.0074872 ,\n",
       "        -0.0009381 ,  0.01571979],\n",
       "       [ 0.02805913, -0.00253367, -0.16755474, ...,  0.01492483,\n",
       "        -0.06151099,  0.04340555]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use sklearn to fit a supervised model based on `model_type`\n",
    "\n",
    "\n",
    "# use sklearn to fit a supervised model based on `model_type`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.03486446)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.abs(Y - X @ (np.linalg.inv(X.T @ X) @ X.T @ Y))).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/njwfish/miniconda3/envs/omnicell/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-01-12 19:57:51,653 - INFO - SCLambda model selected\n"
     ]
    }
   ],
   "source": [
    "from train import get_model\n",
    "model = get_model(config.get_model_name(), config.model_config, loader, pert_rep_map, input_dim, device, pert_ids, gene_emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-12 19:59:05,108 - INFO - Computing 32000-dimensional perturbation embeddings for 279630 cells...\n",
      "/home/njwfish/miniconda3/envs/omnicell/lib/python3.9/site-packages/anndata/_core/storage.py:85: ImplicitModificationWarning: X should not be a np.matrix, use np.ndarray instead.\n",
      "  warnings.warn(msg, ImplicitModificationWarning)\n",
      "2025-01-12 19:59:35,468 - INFO - Splitting data...\n",
      "  2%|▏         | 4/200 [02:07<1:44:02, 31.85s/it]2025-01-12 20:03:23,463 - INFO - Epoch  5 complete! -  Loss: 3856.907958984375\n",
      "2025-01-12 20:06:54,662 - INFO - Validation correlation delta 0.23106926226955496\n",
      "  4%|▍         | 9/200 [08:16<2:31:07, 47.48s/it] 2025-01-12 20:09:32,692 - INFO - Epoch  10 complete! -  Loss: nan\n",
      "2025-01-12 20:13:08,032 - INFO - Validation correlation delta nan\n",
      "  7%|▋         | 14/200 [14:31<2:35:28, 50.15s/it] 2025-01-12 20:15:48,229 - INFO - Epoch  15 complete! -  Loss: nan\n",
      "2025-01-12 20:19:25,387 - INFO - Validation correlation delta nan\n",
      " 10%|▉         | 19/200 [20:49<2:32:53, 50.68s/it] 2025-01-12 20:22:05,256 - INFO - Epoch  20 complete! -  Loss: nan\n",
      "2025-01-12 20:25:39,663 - INFO - Validation correlation delta nan\n",
      " 12%|█▏        | 24/200 [27:03<2:28:12, 50.53s/it] 2025-01-12 20:28:19,324 - INFO - Epoch  25 complete! -  Loss: nan\n",
      "2025-01-12 20:31:53,659 - INFO - Validation correlation delta nan\n",
      " 14%|█▍        | 29/200 [33:17<2:23:56, 50.51s/it] 2025-01-12 20:34:33,415 - INFO - Epoch  30 complete! -  Loss: nan\n",
      "2025-01-12 20:38:07,101 - INFO - Validation correlation delta nan\n",
      " 17%|█▋        | 34/200 [39:30<2:19:38, 50.47s/it] 2025-01-12 20:40:46,870 - INFO - Epoch  35 complete! -  Loss: nan\n",
      "2025-01-12 20:44:21,724 - INFO - Validation correlation delta nan\n",
      " 20%|█▉        | 39/200 [45:45<2:15:29, 50.49s/it] 2025-01-12 20:47:01,145 - INFO - Epoch  40 complete! -  Loss: nan\n",
      "2025-01-12 20:50:34,730 - INFO - Validation correlation delta nan\n",
      " 22%|██▏       | 44/200 [51:57<2:10:53, 50.34s/it] 2025-01-12 20:53:13,768 - INFO - Epoch  45 complete! -  Loss: nan\n",
      "2025-01-12 20:56:47,180 - INFO - Validation correlation delta nan\n",
      " 24%|██▍       | 49/200 [58:10<2:06:50, 50.40s/it] 2025-01-12 20:59:26,922 - INFO - Epoch  50 complete! -  Loss: nan\n",
      "2025-01-12 21:03:02,061 - INFO - Validation correlation delta nan\n",
      " 27%|██▋       | 54/200 [1:04:25<2:03:02, 50.57s/it] 2025-01-12 21:05:41,861 - INFO - Epoch  55 complete! -  Loss: nan\n",
      "2025-01-12 21:09:16,247 - INFO - Validation correlation delta nan\n",
      " 30%|██▉       | 59/200 [1:10:39<1:58:44, 50.53s/it] 2025-01-12 21:11:56,031 - INFO - Epoch  60 complete! -  Loss: nan\n",
      "2025-01-12 21:15:30,881 - INFO - Validation correlation delta nan\n",
      " 32%|███▏      | 64/200 [1:16:54<1:54:31, 50.52s/it] 2025-01-12 21:18:10,506 - INFO - Epoch  65 complete! -  Loss: nan\n",
      "2025-01-12 21:21:44,275 - INFO - Validation correlation delta nan\n",
      " 34%|███▍      | 69/200 [1:23:07<1:50:07, 50.44s/it] 2025-01-12 21:24:23,723 - INFO - Epoch  70 complete! -  Loss: nan\n",
      "2025-01-12 21:27:58,555 - INFO - Validation correlation delta nan\n",
      " 37%|███▋      | 74/200 [1:29:21<1:46:02, 50.49s/it] 2025-01-12 21:30:38,118 - INFO - Epoch  75 complete! -  Loss: nan\n",
      "2025-01-12 21:34:12,531 - INFO - Validation correlation delta nan\n",
      " 40%|███▉      | 79/200 [1:35:36<1:41:53, 50.52s/it] 2025-01-12 21:36:52,229 - INFO - Epoch  80 complete! -  Loss: nan\n",
      "2025-01-12 21:40:26,345 - INFO - Validation correlation delta nan\n",
      " 42%|████▏     | 84/200 [1:41:49<1:37:33, 50.46s/it] 2025-01-12 21:43:05,956 - INFO - Epoch  85 complete! -  Loss: nan\n",
      "2025-01-12 21:46:39,476 - INFO - Validation correlation delta nan\n",
      " 44%|████▍     | 89/200 [1:48:02<1:33:16, 50.42s/it] 2025-01-12 21:49:19,087 - INFO - Epoch  90 complete! -  Loss: nan\n",
      "2025-01-12 21:52:53,846 - INFO - Validation correlation delta nan\n",
      " 47%|████▋     | 94/200 [1:54:17<1:29:14, 50.51s/it] 2025-01-12 21:55:33,474 - INFO - Epoch  95 complete! -  Loss: nan\n",
      "2025-01-12 21:59:08,097 - INFO - Validation correlation delta nan\n",
      " 50%|████▉     | 99/200 [2:00:31<1:25:02, 50.52s/it] 2025-01-12 22:01:47,891 - INFO - Epoch  100 complete! -  Loss: nan\n",
      "2025-01-12 22:05:21,280 - INFO - Validation correlation delta nan\n",
      " 52%|█████▏    | 104/200 [2:06:44<1:20:40, 50.42s/it] 2025-01-12 22:08:00,900 - INFO - Epoch  105 complete! -  Loss: nan\n",
      "2025-01-12 22:11:34,816 - INFO - Validation correlation delta nan\n",
      " 55%|█████▍    | 109/200 [2:12:58<1:16:29, 50.43s/it] 2025-01-12 22:14:14,420 - INFO - Epoch  110 complete! -  Loss: nan\n",
      "2025-01-12 22:17:49,122 - INFO - Validation correlation delta nan\n",
      " 57%|█████▋    | 114/200 [2:19:12<1:12:20, 50.47s/it] 2025-01-12 22:20:28,524 - INFO - Epoch  115 complete! -  Loss: nan\n",
      "2025-01-12 22:24:03,534 - INFO - Validation correlation delta nan\n",
      " 60%|█████▉    | 119/200 [2:25:27<1:08:13, 50.53s/it] 2025-01-12 22:26:43,275 - INFO - Epoch  120 complete! -  Loss: nan\n",
      "2025-01-12 22:30:17,949 - INFO - Validation correlation delta nan\n",
      " 62%|██████▏   | 124/200 [2:31:41<1:04:00, 50.53s/it] 2025-01-12 22:32:57,631 - INFO - Epoch  125 complete! -  Loss: nan\n",
      "2025-01-12 22:36:32,420 - INFO - Validation correlation delta nan\n",
      " 64%|██████▍   | 129/200 [2:37:55<59:41, 50.44s/it]   2025-01-12 22:39:11,475 - INFO - Epoch  130 complete! -  Loss: nan\n",
      "2025-01-12 22:42:44,663 - INFO - Validation correlation delta nan\n",
      " 67%|██████▋   | 134/200 [2:44:07<55:18, 50.28s/it]   2025-01-12 22:45:23,704 - INFO - Epoch  135 complete! -  Loss: nan\n",
      "2025-01-12 22:48:56,925 - INFO - Validation correlation delta nan\n",
      " 70%|██████▉   | 139/200 [2:50:20<51:06, 50.27s/it]   2025-01-12 22:51:36,108 - INFO - Epoch  140 complete! -  Loss: nan\n",
      "2025-01-12 22:55:09,499 - INFO - Validation correlation delta nan\n",
      " 72%|███████▏  | 144/200 [2:56:32<46:58, 50.33s/it]   2025-01-12 22:57:48,819 - INFO - Epoch  145 complete! -  Loss: nan\n",
      "2025-01-12 23:01:54,549 - INFO - Validation correlation delta nan\n",
      " 74%|███████▍  | 149/200 [3:03:19<45:07, 53.09s/it]   2025-01-12 23:04:36,696 - INFO - Epoch  150 complete! -  Loss: nan\n",
      "2025-01-12 23:08:48,566 - INFO - Validation correlation delta nan\n",
      " 77%|███████▋  | 154/200 [3:10:14<41:27, 54.07s/it]   2025-01-12 23:11:30,974 - INFO - Epoch  155 complete! -  Loss: nan\n",
      "2025-01-12 23:15:41,006 - INFO - Validation correlation delta nan\n",
      " 80%|███████▉  | 159/200 [3:17:06<36:55, 54.03s/it]   2025-01-12 23:18:23,074 - INFO - Epoch  160 complete! -  Loss: nan\n",
      "2025-01-12 23:22:33,050 - INFO - Validation correlation delta nan\n",
      " 82%|████████▏ | 164/200 [3:23:59<32:30, 54.18s/it]   2025-01-12 23:25:15,818 - INFO - Epoch  165 complete! -  Loss: nan\n",
      "2025-01-12 23:29:29,377 - INFO - Validation correlation delta nan\n",
      " 84%|████████▍ | 169/200 [3:30:55<28:08, 54.48s/it]   2025-01-12 23:32:12,263 - INFO - Epoch  170 complete! -  Loss: nan\n",
      "2025-01-12 23:36:26,919 - INFO - Validation correlation delta nan\n",
      " 87%|████████▋ | 174/200 [3:37:51<23:28, 54.18s/it]   2025-01-12 23:39:07,281 - INFO - Epoch  175 complete! -  Loss: nan\n",
      "2025-01-12 23:42:42,109 - INFO - Validation correlation delta nan\n",
      " 90%|████████▉ | 179/200 [3:44:05<17:54, 51.15s/it] 2025-01-12 23:45:21,765 - INFO - Epoch  180 complete! -  Loss: nan\n",
      "2025-01-12 23:48:55,991 - INFO - Validation correlation delta nan\n",
      " 92%|█████████▏| 184/200 [3:50:19<13:28, 50.56s/it] 2025-01-12 23:51:35,499 - INFO - Epoch  185 complete! -  Loss: nan\n"
     ]
    }
   ],
   "source": [
    "model.train(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omnicell",
   "language": "python",
   "name": "omnicell"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
