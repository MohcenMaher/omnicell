{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "import scvi\n",
    "import torch\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import os\n",
    "\n",
    "import scanpy as sc\n",
    "\n",
    "from pytorch_lightning.callbacks import TQDMProgressBar\n",
    "from datamodules import FMDataset, fm_collate, CFMDataset, SCFMDataset, cfm_collate, StratifiedBatchSampler, ot_collate\n",
    "from torch.utils.data import RandomSampler\n",
    "from sc_etl_utils import *\n",
    "from arch import *\n",
    "from flow_utils import compute_conditional_flow\n",
    "import json\n",
    "\n",
    "import scvi\n",
    "import torch\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import os\n",
    "\n",
    "import scanpy as sc\n",
    "\n",
    "from torchcfm.conditional_flow_matching import *\n",
    "import scanpy as sc\n",
    "import hashlib\n",
    "from llm import MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load some data\n",
    "adata = sc.read_h5ad('/orcd/archive/abugoot/001/Projects/dlesman/datasets/satija_IFNB_HVG_and_perturbed_genes_raw.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_map = {'NT': 0} | {k: i for i, k in enumerate(adata.var.index)}\n",
    "gene_unmap = {gene_map[k]: k for k in gene_map}\n",
    "perts = adata.obs.gene.unique().map(gene_map)\n",
    "adata.obs['pert_type'] = adata.obs.gene.map(gene_map)\n",
    "pert_ids = np.array(adata.obs['pert_type'])\n",
    "pert_mat = np.arange(pert_ids.max() + 1)[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_col = 'cell_type'\n",
    "pert_col = 'pert_type'\n",
    "control_pert, holdout_cells, holdout_perts = 0, ['HT29'], [gene_map['USP18']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Controls: 14582, Perturbations: 313080,  Eval: 880\n"
     ]
    }
   ],
   "source": [
    "from sc_etl_utils import *\n",
    "control_idx, pert_idx, eval_idx, eval_cell_idx, eval_pert_idx = get_train_eval_idxs(\n",
    "    adata, control_pert, holdout_cells, holdout_perts, cell_col=cell_col, pert_col=pert_col\n",
    ")\n",
    "\n",
    "_, _, cell_types = get_identity_features(\n",
    "    adata, cell_col=cell_col, pert_col=pert_col, cell_type_features=False\n",
    ")\n",
    "\n",
    "adata.obsm[\"standard\"] = adata.X\n",
    "X = adata.obsm[\"standard\"]\n",
    "X = X.toarray()\n",
    "X = X / X.sum(axis=1)[:, None]\n",
    "\n",
    "control_train, pert_train, pert_ids_train, control_cell_types, pert_cell_types, control_eval, pert_eval, pert_ids_eval = get_train_eval(\n",
    "    X, pert_ids, cell_types, control_idx, pert_idx, eval_idx, eval_cell_idx, eval_pert_idx\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "base_learning_rate = 5e-5\n",
    "weight_decay=0.0\n",
    "total_epoch = 5000\n",
    "warmup_epoch = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09837271 0.2565357  0.2074058  0.17077354 0.08714468 0.17976757]\n"
     ]
    }
   ],
   "source": [
    "dset = SCFMDataset(\n",
    "    control_train, pert_train, \n",
    "    pert_ids_train, pert_mat, \n",
    "    control_cell_types, pert_cell_types,\n",
    "    batch_size=batch_size, size=X.shape[0]\n",
    ")\n",
    "dl = torch.utils.data.DataLoader(\n",
    "    dset, collate_fn=ot_collate, \n",
    "    batch_sampler=StratifiedBatchSampler(\n",
    "        RandomSampler(dset), batch_size=batch_size, drop_last=True, \n",
    "        probs=dset.probs, num_strata=dset.num_strata\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MAE(X.shape[1])\n",
    "device = 'cuda'\n",
    "# device = 'cpu'\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 5.0000e-05.\n"
     ]
    }
   ],
   "source": [
    "optim = torch.optim.AdamW(model.parameters(), lr=base_learning_rate, betas=(0.9, 0.95), weight_decay=weight_decay)\n",
    "lr_func = lambda epoch: min((epoch + 1) / (warmup_epoch + 1e-5), 0.5 * (math.cos(epoch / total_epoch * math.pi) + 1))\n",
    "lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optim, lr_lambda=lr_func, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_count = 0\n",
    "optim.zero_grad()\n",
    "pert_task = 0\n",
    "for e in range(1):\n",
    "    model.train()\n",
    "    losses = {(0, 0): [], (0, 1): [], (1, 0): [], (1, 1): []}\n",
    "    for (control, pert, pert_index) in (pbar := tqdm(iter(dl))):\n",
    "        control_sparsity = (control > 0).float().to(device)\n",
    "        active_weights = 10 * control_sparsity + 1\n",
    "        control = control.float().to(device)\n",
    "        pert = pert.float().to(device)\n",
    "        step_count += 1\n",
    "        mask_task = step_count % 2\n",
    "        control_recon, sparsity_probs, _ = model(control, mask=mask_task)\n",
    "        loss = torch.sum(active_weights * torch.abs(control_recon - control)) / batch_size\n",
    "        loss += torch.sum(active_weights * torch.abs(control_sparsity - sparsity_probs)) / batch_size\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "        if step_count % 100 == 0:\n",
    "            lr_scheduler.step()\n",
    "        losses[(pert_task, mask_task)].append(loss.item())\n",
    "        if pert_task == 0 and mask_task == 0:\n",
    "            pbar.set_description(\n",
    "                f\"tv: {np.array(losses[(0, 0)])[-100:].mean():.3f}, mtv: {np.array(losses[(0, 1)])[-100:].mean():.3f}\")\n",
    "    \n",
    "    avg_loss = sum(losses[(0, 0)]) / len(losses[(0, 0)])\n",
    "    torch.save(model, f\"transformer_v3.{e}\")\n",
    "    # writer.add_scalar('mae_loss', avg_loss, global_step=e)\n",
    "    print(f'In epoch {e}, average traning loss is {avg_loss}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_count = 0\n",
    "optim.zero_grad()\n",
    "pert_task = 0\n",
    "for e in range(10):\n",
    "    model.train()\n",
    "    losses = {(0, 0): [], (0, 1): [], (1, 0): [], (1, 1): []}\n",
    "    for (control, pert, pert_index) in (pbar := tqdm(iter(dl))):\n",
    "        active_weights = 10 * (control > 0).float().to(device) + 1\n",
    "        control = control.float().to(device)\n",
    "        pert = pert.float().to(device)\n",
    "        step_count += 1\n",
    "        mask_task = step_count % 2\n",
    "        pert_task = (pert_task + mask_task) % 2\n",
    "        if pert_task:\n",
    "            pert_expr = pert[torch.arange(pert.size(0)), pert_index[:, 0], None]\n",
    "            pert_recon, _, _ = model(control, pert_expr=pert_expr, pert_index=pert_index[:, 0], mask=mask_task)\n",
    "            loss = torch.sum(active_weights * torch.abs(pert_recon - pert)) / batch_size\n",
    "        else:\n",
    "            control_recon, _, _ = model(control, mask=mask_task)\n",
    "            loss = torch.sum(active_weights * torch.abs(control_recon - control)) / batch_size\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "        if step_count % 100 == 0:\n",
    "            lr_scheduler.step()\n",
    "        losses[(pert_task, mask_task)].append(loss.item())\n",
    "        if pert_task == 0 and mask_task == 0:\n",
    "            pbar.set_description(\n",
    "                f\"tv: {np.array(losses[(0, 0)])[-100:].mean():.3f}, mtv: {np.array(losses[(0, 1)])[-100:].mean():.3f}, ptv: {np.array(losses[(1, 0)])[-100:].mean():.3f}, pmtv: {np.array(losses[(1, 1)])[-100:].mean():.3f}\")\n",
    "    \n",
    "    avg_loss = sum(losses[(0, 0)]) / len(losses[(0, 0)])\n",
    "    torch.save(model, f\"transformer_v4.{e}\")\n",
    "    # writer.add_scalar('mae_loss', avg_loss, global_step=e)\n",
    "    print(f'In epoch {e}, average traning loss is {avg_loss}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_count = 0\n",
    "optim.zero_grad()\n",
    "pert_task = 0\n",
    "for e in range(10):\n",
    "    model.train()\n",
    "    losses = {(0, 0): [], (0, 1): [], (1, 0): [], (1, 1): []}\n",
    "    for (control, pert, pert_index) in (pbar := tqdm(iter(dl))):\n",
    "        active_weights = 1 # 10 * (control > 0).float().to(device) + 1\n",
    "        control = control.float().to(device)\n",
    "        pert = pert.float().to(device)\n",
    "        step_count += 1\n",
    "        mask_task = step_count % 2\n",
    "        pert_task = (pert_task + mask_task) % 2\n",
    "        if pert_task:\n",
    "            pert_expr = pert[torch.arange(pert.size(0)), pert_index[:, 0], None]\n",
    "            pert_recon, _, _ = model(control, pert_expr=pert_expr, pert_index=pert_index[:, 0], mask=mask_task)\n",
    "            loss = torch.sum(active_weights * torch.abs(pert_recon - pert)) / batch_size\n",
    "        else:\n",
    "            control_recon, _, _ = model(control, mask=mask_task)\n",
    "            loss = torch.sum(active_weights * torch.abs(control_recon - control)) / batch_size\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "        if step_count % 100 == 0:\n",
    "            lr_scheduler.step()\n",
    "        losses[(pert_task, mask_task)].append(loss.item())\n",
    "        if pert_task == 0 and mask_task == 0:\n",
    "            pbar.set_description(\n",
    "                f\"tv: {np.array(losses[(0, 0)])[-100:].mean():.3f}, mtv: {np.array(losses[(0, 1)])[-100:].mean():.3f}, ptv: {np.array(losses[(1, 0)])[-100:].mean():.3f}, pmtv: {np.array(losses[(1, 1)])[-100:].mean():.3f}\")\n",
    "    \n",
    "    avg_loss = sum(losses[(0, 0)]) / len(losses[(0, 0)])\n",
    "    torch.save(model, f\"transformer_v5.{e}\")\n",
    "    # writer.add_scalar('mae_loss', avg_loss, global_step=e)\n",
    "    print(f'In epoch {e}, average traning loss is {avg_loss}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_type_names = adata.obs[cell_col]\n",
    "pert_type_names = adata.obs[pert_col]\n",
    "# Save the predicted perturbation\n",
    "for cell_type, pert_type in zip(holdout_cells, holdout_perts):\n",
    "    control_eval = X[cell_type_names == cell_type]\n",
    "    true_pert= X[(pert_type_names == pert_type) & (cell_type_names == cell_type)]\n",
    "    # cheating right now, just trying to get a mvp together\n",
    "    pert_expr = true_pert[torch.arange(true_pert.size(0)), pert_type, None]\n",
    "    pred_pert, _, _ = model(\n",
    "        control, pert_expr=pert_expr, pert_index=pert_type, mask=False\n",
    "    )\n",
    "    print(f\"Saving {pert_type} predictions\")\n",
    "    np.savez(\n",
    "        f\"Satija_IFNB_HVG/llm/pred_{gene_unmap[pert_type]}_{cell_type}.npz\", \n",
    "        pred_pert=pred_pert[:, hvg_idx].cpu().detach().numpy(), \n",
    "        true_pert=true_pert[:, hvg_idx], \n",
    "        control=control_eval[:, hvg_idx],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsbm",
   "language": "python",
   "name": "dsbm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
